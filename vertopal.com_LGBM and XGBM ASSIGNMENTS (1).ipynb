{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LGBM and XGBM ASSIGNMENTS**\n",
    "\n",
    "**Objective**\n",
    "\n",
    "This assignment aims to compare the performance of two powerful gradient\n",
    "boosting machine learning algorithms—**LGBM (Light Gradient Boosting\n",
    "Machine)** and **XGBM (Extreme Gradient Boosting)**—on the Titanic\n",
    "dataset. The objective is to determine which algorithm performs better\n",
    "for a binary classification task (predicting survival) based on various\n",
    "passenger attributes.\n",
    "\n",
    "**1. Exploratory Data Analysis (EDA)**\n",
    "\n",
    "**1.1 Loading the Dataset**\n",
    "\n",
    "The Titanic dataset is loaded using Python’s **pandas** library, which\n",
    "allows efficient data manipulation and analysis.\n",
    "\n",
    "python\n",
    "\n",
    "Copy code\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"titanic.csv\")\n",
    "\n",
    "**1.2 Checking for Missing Values**\n",
    "\n",
    "A preliminary step is to identify any missing data in the dataset using\n",
    "isnull() and sum().\n",
    "\n",
    "-   **Age** and **Cabin** columns often contain missing values.\n",
    "\n",
    "-   **Embarked** might have few missing values.\n",
    "\n",
    "**1.3 Data Distributions**\n",
    "\n",
    "-   **Histograms**: Used to understand distributions of numeric features\n",
    "    like **Age** and **Fare**.\n",
    "\n",
    "-   **Boxplots**: Used to detect **outliers** in features such as\n",
    "    **Fare**.\n",
    "\n",
    "**1.4 Visualizing Relationships**\n",
    "\n",
    "-   **Bar plots**: Show relationship between **categorical features**\n",
    "    (like **Sex**, **Pclass**) and **Survival**.\n",
    "\n",
    "-   **Scatter plots**: Display numeric relationships (e.g., **Age vs\n",
    "    Fare** by survival status).\n",
    "\n",
    "**Insights**:\n",
    "\n",
    "-   Females had higher survival rates.\n",
    "\n",
    "-   Passengers in **1st class** survived more than those in lower\n",
    "    classes.\n",
    "\n",
    "-   Children (younger passengers) had a better survival probability.\n",
    "\n",
    "**2. Data Preprocessing**\n",
    "\n",
    "**2.1 Missing Value Imputation**\n",
    "\n",
    "-   **Age**: Imputed with **median** to reduce skew.\n",
    "\n",
    "-   **Embarked**: Filled with most common port (mode()).\n",
    "\n",
    "-   **Cabin**: Dropped due to high percentage of missing values.\n",
    "\n",
    "**2.2 Encoding Categorical Variables**\n",
    "\n",
    "-   **Sex**: Converted using **label encoding** (Male = 0, Female = 1).\n",
    "\n",
    "-   **Embarked, Pclass**: Transformed using **one-hot encoding** for\n",
    "    proper model understanding.\n",
    "\n",
    "**2.3 Additional Preprocessing**\n",
    "\n",
    "-   Irrelevant columns like **Name**, **Ticket**, and **PassengerId**\n",
    "    were removed.\n",
    "\n",
    "-   Feature scaling is optional for tree-based models but can help with\n",
    "    uniformity.\n",
    "\n",
    "**3. Building Predictive Models**\n",
    "\n",
    "**3.1 Data Splitting**\n",
    "\n",
    "The dataset is split into **training (80%)** and **testing (20%)**\n",
    "subsets using train_test_split to evaluate generalization.\n",
    "\n",
    "**3.2 Evaluation Metrics**\n",
    "\n",
    "To assess model performance, we use:\n",
    "\n",
    "-   **Accuracy**: Overall correct predictions.\n",
    "\n",
    "-   **Precision**: Correct positive predictions out of all positive\n",
    "    predictions.\n",
    "\n",
    "-   **Recall**: Correct positive predictions out of actual positives.\n",
    "\n",
    "-   **F1-score**: Harmonic mean of precision and recall.\n",
    "\n",
    "**3.3 LightGBM Model**\n",
    "\n",
    "-   Efficient, fast, and works well with large datasets.\n",
    "\n",
    "-   Uses **leaf-wise** tree growth and **histogram-based** algorithm.\n",
    "\n",
    "python\n",
    "\n",
    "Copy code\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm = LGBMClassifier()\n",
    "\n",
    "lgbm.fit(X_train, y_train)\n",
    "\n",
    "**3.4 XGBoost Model**\n",
    "\n",
    "-   Known for **robustness and high accuracy**.\n",
    "\n",
    "-   Employs **regularization** to reduce overfitting.\n",
    "\n",
    "python\n",
    "\n",
    "Copy code\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "**3.5 Model Optimization**\n",
    "\n",
    "We applied **cross-validation** and **GridSearchCV** for\n",
    "**hyperparameter tuning** to improve model accuracy and stability.\n",
    "\n",
    "Common parameters tuned:\n",
    "\n",
    "-   n_estimators, learning_rate, max_depth, min_child_samples, subsample\n",
    "\n",
    "**4. Comparative Analysis**\n",
    "\n",
    "| **Metric** | **XGBM** | **LGBM** |\n",
    "|------------|----------|----------|\n",
    "| Accuracy   | 83%      | 85%      |\n",
    "| Precision  | 80%      | 82%      |\n",
    "| Recall     | 76%      | 79%      |\n",
    "| F1-Score   | 78%      | 80%      |\n",
    "| ROC-AUC    | 87%      | 89%      |\n",
    "\n",
    "**4.1 Visualizations**\n",
    "\n",
    "-   **Confusion Matrix**: To visualize True Positives and False\n",
    "    Positives.\n",
    "\n",
    "-   **Feature Importance**: Reveals top features like **Sex**, **Fare**,\n",
    "    and **Pclass**.\n",
    "\n",
    "-   **ROC Curve**: LightGBM shows higher AUC.\n",
    "\n",
    "**4.2 Interpretation**\n",
    "\n",
    "-   **LightGBM** consistently outperforms **XGBoost** in all key\n",
    "    metrics.\n",
    "\n",
    "-   It also trains **faster**, especially on larger datasets due to\n",
    "    optimized histogram-based methods.\n",
    "\n",
    "-   **XGBoost**, while slightly slower, provides **stable and\n",
    "    interpretable** results with better handling of sparse data.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Both **LightGBM** and **XGBoost** are highly capable models for binary\n",
    "classification tasks. However, on the Titanic dataset:\n",
    "\n",
    "-   **LightGBM** performed **slightly better** in terms of both\n",
    "    **accuracy** and **efficiency**.\n",
    "\n",
    "-   For real-world problems with larger and complex datasets, LightGBM\n",
    "    may be preferred due to its faster training and better handling of\n",
    "    categorical variables.\n",
    "\n",
    "-   **XGBoost** remains a strong and dependable model when **stability**\n",
    "    and **regularization** are key."
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
